{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill out the information of your group!\n",
    "\n",
    "| <p style=\"text-align: center;\">First Name</p>  | <p style=\"text-align: center;\">Family Name</p> | Matr.-No. |\n",
    "| ---------------------------------------------- | ---------------------------------------------- | -------- |\n",
    "| <p style=\"text-align: left\">*EDIT!*</p>| <p style=\"text-align: left\">*EDIT!*</p> | *EDIT!* |\n",
    "| <p style=\"text-align: left\">Michael</p>| <p style=\"text-align: left\">Weikl</p> | k01154652 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center\">344.105/6/7 UE: Natural Language Processing (WS2021/22)</h2>\n",
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 2</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Getting to Know Word Embedding!</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Terms of Use</b><br>\n",
    "This  material is prepared for educational purposes at the Johannes Kepler University Linz (JKU), and is exclusively provided to the registered students of the mentioned course at JKU. It is strictly forbidden to distribute the current file, the contents of the assignment, and its solution. The use or reproduction of this manuscript is only allowed for educational purposes in non-profit organizations, while in this case, the explicit prior acceptance of the author(s) is required.\n",
    "\n",
    "**Author:** Navid Rekab-saz<br>\n",
    "**Email:** navid.rekabsaz@jku.at<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of contents</h2>\n",
    "<ol>\n",
    "    <a href=\"#section-general-guidelines\"><li style=\"font-size:large;font-weight:bold\">General Guidelines</li></a>\n",
    "    <a href=\"#section-taskA\"><li style=\"font-size:large;font-weight:bold\">Task A: Similarity, Nearest Neighbors, and WE Evaluation (20 points)</li></a>\n",
    "    <a href=\"#section-taskB\"><li style=\"font-size:large;font-weight:bold\">Task B: Document Classification with WE (10 points)</li></a>\n",
    "    <a href=\"#section-taskC\"><li style=\"font-size:large;font-weight:bold\">Task C: Classification with sent2vec Document Embeddings (2 extra point)</li></a>\n",
    "    <a href=\"#section-references\"><li style=\"font-size:large;font-weight:bold\">References</li></a>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-general-guidelines\"></a><h2 style=\"color:rgb(0,120,170)\">General Guidelines</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Assignment objective\n",
    "The aim of this assignment is to get familiarized with using word embedding (WE) models in practice. The assignment in total has **30 points**; it also offers **2 extra points** which can cover any missing point.\n",
    "\n",
    "This Notebook encompasses all aspects of the assignment, namely the descriptions of tasks as well as your solutions and reports. Feel free to add any required cell for solutions. The cells can contain code, reports, charts, tables, or any other material, required for the assignment. Feel free to provide the solutions in an interactive and visual way! \n",
    "\n",
    "Please discuss any unclear point in the assignment in the provided forum in MOODLE. It is also encouraged to provide answers to your peer's questions. However when submitting a post, keep in mind to avoid providing solutions. Please let the tutor(s) know shall you find any error or unclarity in the assignment.\n",
    "\n",
    "### Implementation & Libraries\n",
    "\n",
    "The assignment should be implemented with recent versions of `Python` (>3.7). Any standard Python library can be used, so far that the library is free and can be simply installed using `pip` or `conda`. Examples of potentially useful libraries are `scikit-learn`, `numpy`, `scipy`, `gensim`, `nltk`, `spaCy`, and `AllenNLP`. Use the latest stable version of each library.\n",
    "\n",
    "### Submission\n",
    "\n",
    "Each group submits one Notebook file (`.ipynb`) through MOODLE. Do not forget to put in your names and student numbers in the first cell of the Notebook. **In the submitted Notebook, all the results and visualizations should already be present, and can be observed simply by loading the Notebook in a browser.** The Notebook must be self-contained, meaning that one can run all the cells from top to bottom without any error. If you need to include extra files in the submission, compress all files (together with the Notebook) in a `zip` file and submit the zip file to MOODLE. You do not need to include the data files in the submission.\n",
    "\n",
    "Cover the questions/points, mentioned in the tasks, but also add any necessary point for understanding your experiments.  \n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "To conduct the experiments, two datasets are provided. The datasets are taken from the data of `thedeep` project, produced by the DEEP (https://www.thedeep.io) platform. The DEEP is an open-source platform, which aims to facilitate processing of textual data for international humanitarian response organizations. The platform enables the classification of text excerpts, extracted from news and reports into a set of domain specific classes. The provided dataset has 12 classes (labels) like agriculture, health, and protection. \n",
    "\n",
    "The difference between the datasets is in their sizes. We refer to these as `medium` and `small`, containing an overall number of 38,000 and 12,000 annotated text excerpts, respectively. Select one of the datasets, and use it for all of the tasks. `medium` provides more data and therefore reflects a more realistic scenario. `small` is however provided for the sake of convenience, particularly if running the experiments on your available hardware takes too long. Using `medium` is generally recommended, but from the point of view of assignment grading, there is no difference between the datasets.\n",
    "\n",
    "Download the dataset from [this link](https://drive.jku.at/filr/public-link/file-download/0cce88f07c9c862b017c9cfba294077a/33590/5792942781153185740/nlp2021_22_data.zip).\n",
    "\n",
    "Whether `medium` or `small`, you will find the following files in the provided zip file:\n",
    "- `thedeep.$name$.train.txt`: Train set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.validation.txt`: Validation set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.test.txt`: Test set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.label.txt`: Captions of the labels.\n",
    "- `README.txt`: Terms of use of the dataset.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-taskA\"></a><h2 style=\"color:rgb(0,120,170)\">Task A: Similarity, Nearest Neighbors, and WE Evaluation (20 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Loading a word embedding (WE) model (2 points).** Download a pre-trained word embedding model such as word2vec (https://code.google.com/archive/p/word2vec/) or GloVe (https://nlp.stanford.edu/projects/glove/). You can load the downloaded vectors into arrays, or use libraries such as `gensim` to download and process the vectors. \n",
    "\n",
    "**Calculating word-to-word similarities (3 points).** Select <ins>3 arbitrary words</ins>, referred to as *source words*. For each source word, calculate the cosine similarities to the <ins>5 other words</ins>, that from your own linguistic judgement have various levels of semantic relations to the source word (from highly related to not related at all). Compare the output similarities with your judgements on the semantic relations of words, and report your observations. Consider that when calculating the cosine similarities, DO NOT use the provided functionalities of any library, but implement the cosine similarity as a function that takes two vectors and returns a similarity score.\n",
    "\n",
    "**Calculating nearest neighbors (7 points).** For the selected source words, retrieve the 10 nearest neighbors in the word embedding model, namely the words that have the highest similarities to the source word. Consider that when calculating the nearest neighbors, DO NOT use the provided functionalities of any library but implement it as the function that takes a source vector, a set of target vectors, and a $k$ parameter, and returns the $k$ nearest neighbors and their similarity scores. This function should provide an *efficient* calculation of nearest neighbors. An inefficient way (which should be avoided!) would be looping over the set of vectors in the word embedding model, and one by one calculating the cosine similarity of a source vector to each of the target vectors. As a hint for an efficient way, consider that in `numpy` (and other libraries), calculating the dot product of a vector to a matrix is much faster than the dot products of the vector to each vector of the matrix. (**3 out of 7 points for efficiency**)\n",
    "\n",
    "**WE evaluation (8 points).** There have been several efforts in devising benchmarking datasets to intrinsically evaluate the *goodness* of word embedding models. [Levy et al.](https://aclanthology.org/Q15-1016.pdf) [1] in Section 4.3 describe the two evaluation category of *Word Similarity* and *Analogy* tasks, reference the various datasets of each task, explain the method(s) to calculate the required quantities, and finally discribes the method to evaluate the models on the tasks. Following the descriptions in the paper, select one word similarity and one analogy dataset, download the corresponding datasets (the datasets are publicly available –– they can also be found through the cited papers), and conduct the evaluation process on the WE model. For the analogy task, conduct the experiments using both `3CosAdd` and `3CosMul` methods. Report the evaluation results. Your results should be in the range of the ones reported in the paper (for sanity check). As before, provide your own implementation of the calculations and DO NOT use the functions of libraries.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from os import system\n",
    "import requests\n",
    "import zipfile, tarfile\n",
    "import io\n",
    "import pathlib\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_FILE = \"glove/glove.6B.100d.txt\"\n",
    "WORDSIM_FILE = \"wordsim/wordsim_similarity_goldstandard.txt\"\n",
    "ANALOGY_FILE = \"analogy/word-test.v1.txt\"\n",
    "\n",
    "DOWNLOAD_GLOVE = False\n",
    "# DOWNLOAD_WORDSIM = True\n",
    "# DOWNLOAD_ANALOGY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, fname: str, stream=True, unit_divisor=1024):\n",
    "    resp = requests.get(url, stream=stream)\n",
    "    \n",
    "    if stream:\n",
    "        total = int(resp.headers.get('content-length', 0))\n",
    "        print(total)\n",
    "        with open(fname, 'wb') as file, tqdm(\\\n",
    "            desc=fname,\\\n",
    "            total=total,\\\n",
    "            unit='iB',\\\n",
    "            unit_scale=True,\\\n",
    "            unit_divisor=unit_divisor,\\\n",
    "        ) as bar:\n",
    "            for data in resp.iter_content(chunk_size=unit_divisor):\n",
    "                size = file.write(data)\n",
    "                bar.update(size)\n",
    "    else:\n",
    "        with open(fname, \"wb\") as file:\n",
    "            file.write(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_GLOVE:\n",
    "    glove_dir = pathlib.Path(\"./glove\")\n",
    "    if glove_dir.exists():\n",
    "        shutil.rmtree(glove_dir)\n",
    "    glove_dir.mkdir(parents=False, exist_ok=False)\n",
    "    \n",
    "    zip_path = \"./glove/glove.zip\"\n",
    "    download(url=\"https://nlp.stanford.edu/data/glove.6B.zip\", fname=zip_path)\n",
    "    zip_file = zipfile.ZipFile(zip_path, 'r')\n",
    "    zip_file.extractall(glove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.038194</td>\n",
       "      <td>-0.244870</td>\n",
       "      <td>0.728120</td>\n",
       "      <td>-0.399610</td>\n",
       "      <td>0.083172</td>\n",
       "      <td>0.043953</td>\n",
       "      <td>-0.391410</td>\n",
       "      <td>0.334400</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>0.087459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016215</td>\n",
       "      <td>-0.017099</td>\n",
       "      <td>-0.389840</td>\n",
       "      <td>0.87424</td>\n",
       "      <td>-0.725690</td>\n",
       "      <td>-0.510580</td>\n",
       "      <td>-0.520280</td>\n",
       "      <td>-0.145900</td>\n",
       "      <td>0.82780</td>\n",
       "      <td>0.270620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.107670</td>\n",
       "      <td>0.110530</td>\n",
       "      <td>0.598120</td>\n",
       "      <td>-0.543610</td>\n",
       "      <td>0.673960</td>\n",
       "      <td>0.106630</td>\n",
       "      <td>0.038867</td>\n",
       "      <td>0.354810</td>\n",
       "      <td>0.06351</td>\n",
       "      <td>-0.094189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349510</td>\n",
       "      <td>-0.722600</td>\n",
       "      <td>0.375490</td>\n",
       "      <td>0.44410</td>\n",
       "      <td>-0.990590</td>\n",
       "      <td>0.612140</td>\n",
       "      <td>-0.351110</td>\n",
       "      <td>-0.831550</td>\n",
       "      <td>0.45293</td>\n",
       "      <td>0.082577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-0.339790</td>\n",
       "      <td>0.209410</td>\n",
       "      <td>0.463480</td>\n",
       "      <td>-0.647920</td>\n",
       "      <td>-0.383770</td>\n",
       "      <td>0.038034</td>\n",
       "      <td>0.171270</td>\n",
       "      <td>0.159780</td>\n",
       "      <td>0.46619</td>\n",
       "      <td>-0.019169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063351</td>\n",
       "      <td>-0.674120</td>\n",
       "      <td>-0.068895</td>\n",
       "      <td>0.53604</td>\n",
       "      <td>-0.877730</td>\n",
       "      <td>0.318020</td>\n",
       "      <td>-0.392420</td>\n",
       "      <td>-0.233940</td>\n",
       "      <td>0.47298</td>\n",
       "      <td>-0.028803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.152900</td>\n",
       "      <td>-0.242790</td>\n",
       "      <td>0.898370</td>\n",
       "      <td>0.169960</td>\n",
       "      <td>0.535160</td>\n",
       "      <td>0.487840</td>\n",
       "      <td>-0.588260</td>\n",
       "      <td>-0.179820</td>\n",
       "      <td>-1.35810</td>\n",
       "      <td>0.425410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187120</td>\n",
       "      <td>-0.018488</td>\n",
       "      <td>-0.267570</td>\n",
       "      <td>0.72700</td>\n",
       "      <td>-0.593630</td>\n",
       "      <td>-0.348390</td>\n",
       "      <td>-0.560940</td>\n",
       "      <td>-0.591000</td>\n",
       "      <td>1.00390</td>\n",
       "      <td>0.206640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.189700</td>\n",
       "      <td>0.050024</td>\n",
       "      <td>0.190840</td>\n",
       "      <td>-0.049184</td>\n",
       "      <td>-0.089737</td>\n",
       "      <td>0.210060</td>\n",
       "      <td>-0.549520</td>\n",
       "      <td>0.098377</td>\n",
       "      <td>-0.20135</td>\n",
       "      <td>0.342410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131340</td>\n",
       "      <td>0.058617</td>\n",
       "      <td>-0.318690</td>\n",
       "      <td>-0.61419</td>\n",
       "      <td>-0.623930</td>\n",
       "      <td>-0.415480</td>\n",
       "      <td>-0.038175</td>\n",
       "      <td>-0.398040</td>\n",
       "      <td>0.47647</td>\n",
       "      <td>-0.159830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chanty</th>\n",
       "      <td>-0.155770</td>\n",
       "      <td>-0.049188</td>\n",
       "      <td>-0.064377</td>\n",
       "      <td>0.223600</td>\n",
       "      <td>-0.201460</td>\n",
       "      <td>-0.038963</td>\n",
       "      <td>0.129710</td>\n",
       "      <td>-0.294510</td>\n",
       "      <td>0.00359</td>\n",
       "      <td>-0.098377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093324</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>-0.023469</td>\n",
       "      <td>-0.48099</td>\n",
       "      <td>0.623320</td>\n",
       "      <td>0.024318</td>\n",
       "      <td>-0.275870</td>\n",
       "      <td>0.075044</td>\n",
       "      <td>-0.56380</td>\n",
       "      <td>0.145010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kronik</th>\n",
       "      <td>-0.094426</td>\n",
       "      <td>0.147250</td>\n",
       "      <td>-0.157390</td>\n",
       "      <td>0.071966</td>\n",
       "      <td>-0.298450</td>\n",
       "      <td>0.039432</td>\n",
       "      <td>0.021870</td>\n",
       "      <td>0.008041</td>\n",
       "      <td>-0.18682</td>\n",
       "      <td>-0.311010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.305450</td>\n",
       "      <td>-0.011082</td>\n",
       "      <td>0.118550</td>\n",
       "      <td>-0.11312</td>\n",
       "      <td>0.339510</td>\n",
       "      <td>-0.224490</td>\n",
       "      <td>0.257430</td>\n",
       "      <td>0.631430</td>\n",
       "      <td>-0.20090</td>\n",
       "      <td>-0.105420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rolonda</th>\n",
       "      <td>0.360880</td>\n",
       "      <td>-0.169190</td>\n",
       "      <td>-0.327040</td>\n",
       "      <td>0.098332</td>\n",
       "      <td>-0.429700</td>\n",
       "      <td>-0.188740</td>\n",
       "      <td>0.455560</td>\n",
       "      <td>0.285290</td>\n",
       "      <td>0.30340</td>\n",
       "      <td>-0.366830</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044082</td>\n",
       "      <td>0.140030</td>\n",
       "      <td>0.300070</td>\n",
       "      <td>-0.12731</td>\n",
       "      <td>-0.143040</td>\n",
       "      <td>-0.069396</td>\n",
       "      <td>0.281600</td>\n",
       "      <td>0.271390</td>\n",
       "      <td>-0.29188</td>\n",
       "      <td>0.161090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsombor</th>\n",
       "      <td>-0.104610</td>\n",
       "      <td>-0.504700</td>\n",
       "      <td>-0.493310</td>\n",
       "      <td>0.135160</td>\n",
       "      <td>-0.363710</td>\n",
       "      <td>-0.447500</td>\n",
       "      <td>0.184290</td>\n",
       "      <td>-0.056510</td>\n",
       "      <td>0.40474</td>\n",
       "      <td>-0.725830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151530</td>\n",
       "      <td>-0.108420</td>\n",
       "      <td>0.340640</td>\n",
       "      <td>-0.40916</td>\n",
       "      <td>-0.081263</td>\n",
       "      <td>0.095315</td>\n",
       "      <td>0.150180</td>\n",
       "      <td>0.425270</td>\n",
       "      <td>-0.51250</td>\n",
       "      <td>-0.170540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sandberger</th>\n",
       "      <td>0.283650</td>\n",
       "      <td>-0.626300</td>\n",
       "      <td>-0.443510</td>\n",
       "      <td>0.217700</td>\n",
       "      <td>-0.087421</td>\n",
       "      <td>-0.170620</td>\n",
       "      <td>0.292660</td>\n",
       "      <td>-0.024899</td>\n",
       "      <td>0.26414</td>\n",
       "      <td>-0.170230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138850</td>\n",
       "      <td>-0.228620</td>\n",
       "      <td>0.071792</td>\n",
       "      <td>-0.43208</td>\n",
       "      <td>0.539800</td>\n",
       "      <td>-0.085806</td>\n",
       "      <td>0.032651</td>\n",
       "      <td>0.436780</td>\n",
       "      <td>-0.82607</td>\n",
       "      <td>-0.157010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1         2         3         4         5         6    \\\n",
       "word                                                                     \n",
       "the        -0.038194 -0.244870  0.728120 -0.399610  0.083172  0.043953   \n",
       ",          -0.107670  0.110530  0.598120 -0.543610  0.673960  0.106630   \n",
       ".          -0.339790  0.209410  0.463480 -0.647920 -0.383770  0.038034   \n",
       "of         -0.152900 -0.242790  0.898370  0.169960  0.535160  0.487840   \n",
       "to         -0.189700  0.050024  0.190840 -0.049184 -0.089737  0.210060   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "chanty     -0.155770 -0.049188 -0.064377  0.223600 -0.201460 -0.038963   \n",
       "kronik     -0.094426  0.147250 -0.157390  0.071966 -0.298450  0.039432   \n",
       "rolonda     0.360880 -0.169190 -0.327040  0.098332 -0.429700 -0.188740   \n",
       "zsombor    -0.104610 -0.504700 -0.493310  0.135160 -0.363710 -0.447500   \n",
       "sandberger  0.283650 -0.626300 -0.443510  0.217700 -0.087421 -0.170620   \n",
       "\n",
       "                 7         8        9         10   ...       91        92   \\\n",
       "word                                               ...                       \n",
       "the        -0.391410  0.334400 -0.57545  0.087459  ...  0.016215 -0.017099   \n",
       ",           0.038867  0.354810  0.06351 -0.094189  ...  0.349510 -0.722600   \n",
       ".           0.171270  0.159780  0.46619 -0.019169  ... -0.063351 -0.674120   \n",
       "of         -0.588260 -0.179820 -1.35810  0.425410  ...  0.187120 -0.018488   \n",
       "to         -0.549520  0.098377 -0.20135  0.342410  ... -0.131340  0.058617   \n",
       "...              ...       ...      ...       ...  ...       ...       ...   \n",
       "chanty      0.129710 -0.294510  0.00359 -0.098377  ...  0.093324  0.094486   \n",
       "kronik      0.021870  0.008041 -0.18682 -0.311010  ... -0.305450 -0.011082   \n",
       "rolonda     0.455560  0.285290  0.30340 -0.366830  ... -0.044082  0.140030   \n",
       "zsombor     0.184290 -0.056510  0.40474 -0.725830  ...  0.151530 -0.108420   \n",
       "sandberger  0.292660 -0.024899  0.26414 -0.170230  ...  0.138850 -0.228620   \n",
       "\n",
       "                 93       94        95        96        97        98   \\\n",
       "word                                                                    \n",
       "the        -0.389840  0.87424 -0.725690 -0.510580 -0.520280 -0.145900   \n",
       ",           0.375490  0.44410 -0.990590  0.612140 -0.351110 -0.831550   \n",
       ".          -0.068895  0.53604 -0.877730  0.318020 -0.392420 -0.233940   \n",
       "of         -0.267570  0.72700 -0.593630 -0.348390 -0.560940 -0.591000   \n",
       "to         -0.318690 -0.61419 -0.623930 -0.415480 -0.038175 -0.398040   \n",
       "...              ...      ...       ...       ...       ...       ...   \n",
       "chanty     -0.023469 -0.48099  0.623320  0.024318 -0.275870  0.075044   \n",
       "kronik      0.118550 -0.11312  0.339510 -0.224490  0.257430  0.631430   \n",
       "rolonda     0.300070 -0.12731 -0.143040 -0.069396  0.281600  0.271390   \n",
       "zsombor     0.340640 -0.40916 -0.081263  0.095315  0.150180  0.425270   \n",
       "sandberger  0.071792 -0.43208  0.539800 -0.085806  0.032651  0.436780   \n",
       "\n",
       "                99        100  \n",
       "word                           \n",
       "the         0.82780  0.270620  \n",
       ",           0.45293  0.082577  \n",
       ".           0.47298 -0.028803  \n",
       "of          1.00390  0.206640  \n",
       "to          0.47647 -0.159830  \n",
       "...             ...       ...  \n",
       "chanty     -0.56380  0.145010  \n",
       "kronik     -0.20090 -0.105420  \n",
       "rolonda    -0.29188  0.161090  \n",
       "zsombor    -0.51250 -0.170540  \n",
       "sandberger -0.82607 -0.157010  \n",
       "\n",
       "[400000 rows x 100 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pd.read_table(VECTOR_FILE, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "words.index.name = \"word\"\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_from_word(dataframe, word):\n",
    "    try:\n",
    "        result = dataframe.loc[word, 1:].to_numpy()\n",
    "    except KeyError:\n",
    "        result = np.nan\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_embedding_matrix(dataframe):\n",
    "    return dataframe.to_numpy()\n",
    "\n",
    "def compute_cosine_for_vectors(vector_1, vector_2, axis_norm_v1=0, axis_norm_v2=0):\n",
    "    nominator = (vector_1 @ vector_2)\n",
    "    denominator = (np.linalg.norm(vector_1, axis=axis_norm_v1) * np.linalg.norm(vector_2, axis=axis_norm_v2))\n",
    "    return nominator/denominator\n",
    "\n",
    "def compute_cosine_for_words(word1, word2, words):\n",
    "    word_vector_1 = get_embedding_from_word(words, word1)\n",
    "    word_vector_2 = get_embedding_from_word(words, word2)\n",
    "    \n",
    "    if np.isnan(word_vector_1).any() or np.isnan(word_vector_2).any():\n",
    "        return np.nan\n",
    "    \n",
    "    return compute_cosine_for_vectors(word_vector_1, word_vector_2)\n",
    "\n",
    "def compare_cosine_for_base(base_word, comparisson_words, words):\n",
    "    for test_word in comparisson_words:\n",
    "        print(f\"cosine similarity of {base_word} with {test_word} is \"\n",
    "              f\"{round(compute_cosine_for_words(base_word, test_word, words),4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of train with station is 0.6394\n",
      "cosine similarity of train with schedule is 0.4241\n",
      "cosine similarity of train with plane is 0.6457\n",
      "cosine similarity of train with mouse is 0.071\n",
      "cosine similarity of train with tree is 0.2779\n"
     ]
    }
   ],
   "source": [
    "source_word_1 = \"train\"\n",
    "source_word_1_tests = [\"station\",\"schedule\", \"plane\", \"mouse\", \"tree\"]\n",
    "\n",
    "compare_cosine_for_base(source_word_1, source_word_1_tests, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of sun with star is 0.493\n",
      "cosine similarity of sun with light is 0.5582\n",
      "cosine similarity of sun with ray is 0.3676\n",
      "cosine similarity of sun with cold is 0.4737\n",
      "cosine similarity of sun with car is 0.153\n"
     ]
    }
   ],
   "source": [
    "source_word_2 = \"sun\"\n",
    "source_word_2_tests = [\"star\",\"light\", \"ray\", \"cold\", \"car\"]\n",
    "\n",
    "compare_cosine_for_base(source_word_2, source_word_2_tests, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of calculator with calculation is 0.5303\n",
      "cosine similarity of calculator with computer is 0.5005\n",
      "cosine similarity of calculator with symbols is 0.0923\n",
      "cosine similarity of calculator with speed is 0.1852\n",
      "cosine similarity of calculator with whale is 0.0201\n"
     ]
    }
   ],
   "source": [
    "source_word_3 = \"calculator\"\n",
    "source_word_3_tests = [\"calculation\",\"computer\", \"symbols\", \"speed\", \"whale\"]\n",
    "\n",
    "compare_cosine_for_base(source_word_3, source_word_3_tests, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_nearest_neigbours(source_vector, words, k): \n",
    "    B = get_embedding_matrix(words)\n",
    "    \n",
    "    distances = np.linalg.norm(B - source_vector, axis=1)\n",
    "    cos = (B @ source_vector)/(np.linalg.norm(B, axis=1)*np.linalg.norm(source_vector))\n",
    "    nearest_idx = np.argsort(distances)[:k]\n",
    "   \n",
    "    results = []\n",
    "    for i in nearest_idx:\n",
    "        results.append((words.index[i], round(cos[i],4)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('train', 1.0),\n",
       " ('trains', 0.8663),\n",
       " ('bus', 0.8474),\n",
       " ('rail', 0.758),\n",
       " ('taxi', 0.6938),\n",
       " ('passenger', 0.7333),\n",
       " ('buses', 0.7381),\n",
       " ('commuter', 0.7288),\n",
       " ('subway', 0.7134),\n",
       " ('ride', 0.6873)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_nearest_neigbours(get_embedding_from_word(words, source_word_1), words, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sun', 1.0),\n",
       " ('sky', 0.6127),\n",
       " ('moon', 0.6138),\n",
       " ('sunshine', 0.5513),\n",
       " ('bright', 0.557),\n",
       " ('sunrise', 0.5113),\n",
       " ('light', 0.5582),\n",
       " ('dappled', 0.4808),\n",
       " ('earth', 0.5685),\n",
       " ('turned', 0.477)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_nearest_neigbours(get_embedding_from_word(words, source_word_2), words, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('calculator', 1.0),\n",
       " ('calculators', 0.8147),\n",
       " ('graphing', 0.6313),\n",
       " ('gizmo', 0.5081),\n",
       " ('toolbox', 0.4978),\n",
       " ('worksheet', 0.4976),\n",
       " ('wristwatch', 0.4786),\n",
       " ('timer', 0.5637),\n",
       " ('cautery', 0.4003),\n",
       " ('screensaver', 0.4273)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_nearest_neigbours(get_embedding_from_word(words, source_word_3), words, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DOWNLOAD_WORDSIM:\n",
    "#     wordsim_dir = pathlib.Path(\"./wordsim\")\n",
    "#     print(wordsim_dir.exists())\n",
    "#     if wordsim_dir.exists():\n",
    "#         shutil.rmtree(wordsim_dir)\n",
    "#     wordsim_dir.mkdir(parents=False, exist_ok=False)\n",
    "    \n",
    "#     tar_path = \"./wordsim/ws353simrel.tar.gz\"\n",
    "#     download(url=\"http://alfonseca.org/pubs/ws353simrel.tar.gz\", fname=tar_path, stream=False)\n",
    "#     shutil.unpack_archive(tar_path, wordsim_dir)\n",
    "# #     tar_file = tarfile.open(tar_path, 'r:gz')\n",
    "# #     tar_file.extractall(wordsim_dir)\n",
    "\n",
    "# if DOWNLOAD_ANALOGY:\n",
    "#     analogy_dir = pathlib.Path(\"./analogy\")\n",
    "#     if analogy_dir.exists():\n",
    "#         shutil.rmtree(analogy_dir)\n",
    "#     analogy_dir.mkdir(parents=False, exist_ok=False)\n",
    "    \n",
    "#     txt_path = \"./analogy/word-test.v1.txt\"\n",
    "#     download(url=\"http://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt\", fname=txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_array(array):\n",
    "    print(array)\n",
    "    tmp = array.argsort()\n",
    "    print(tmp)\n",
    "    rank = tmp.argsort()\n",
    "    print(rank)\n",
    "    rank += 1\n",
    "    print(rank)\n",
    "    \n",
    "    r1 = rankdata(array)\n",
    "    print(r1)\n",
    "        \n",
    "    \n",
    "    return rank\n",
    "    \n",
    "    \n",
    "def spearman_correlation(x: np.ndarray, y: np.ndarray):\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(f\"Length of given arrays do not match. x: '{len(x)}', y: '{len(y)}'\")\n",
    "    \n",
    "    x_r = rankdata(x)\n",
    "    y_r = rankdata(y)\n",
    "    \n",
    "    x_r2 = x_r**2\n",
    "    y_r2 = y_r**2\n",
    "    \n",
    "    n = len(x)\n",
    "    prod_r1_r2 = x_r * y_r\n",
    "    \n",
    "    correlation = (n * (np.sum(prod_r1_r2))) - (np.sum(x_r) * np.sum(y_r))\n",
    "    std_x_r = np.sqrt((n * (np.sum(x_r2))) - np.sum(x_r)**2)\n",
    "    std_y_r = np.sqrt((n * (np.sum(y_r2))) - np.sum(y_r)**2)\n",
    "    \n",
    "    spearman = correlation / (std_x_r * std_y_r)\n",
    "    return spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word similarity via the WordSim353 - Similarity is: 0.60193\n"
     ]
    }
   ],
   "source": [
    "wordsim_df = pd.read_csv(WORDSIM_FILE, sep='\\t',\n",
    "                         names=[\"word1\", \"word2\", \"rating\"])\n",
    "\n",
    "wordsim_df['cosine_sim'] = wordsim_df.apply(lambda row: compute_cosine_for_words(row['word1'], \\\n",
    "                                                                                 row['word2'], \\\n",
    "                                                                                 words), \\\n",
    "                                            axis = 1)\n",
    "\n",
    "wordsim_df.dropna(subset = [\"cosine_sim\"], inplace=True)\n",
    "sc = spearman_correlation(wordsim_df['cosine_sim'].to_numpy(), wordsim_df['rating'].to_numpy())\n",
    "sc1 = wordsim_df[['rating', 'cosine_sim']].corr(method=\"spearman\")\n",
    "\n",
    "print(f\"The word similarity via the WordSim353 - Similarity is: {sc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy dataset conists of 19557 analogies\n",
      "Analogy dataset contains 9582 analogies, where all 4 parts are also part of the vocabulary\n"
     ]
    }
   ],
   "source": [
    "analogy_df = pd.read_csv(ANALOGY_FILE, sep=' ',\n",
    "                         names=[\"source_antecedent\", \"source_conclusion\", \"target_antecedent\", \"target_conclusion\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocab = words.index.tolist()\n",
    "analogy_df['complete_vocab'] = analogy_df.apply(lambda row: row['source_antecedent'] in vocab and \\\n",
    "                                                            row['source_conclusion'] in vocab and \\\n",
    "                                                            row['target_antecedent'] in vocab and \\\n",
    "                                                            row['target_conclusion'] in vocab\n",
    "                                                , axis = 1)\n",
    "\n",
    "analogy_reduced = analogy_df[analogy_df['complete_vocab']].dropna()\n",
    "print(f\"Analogy dataset conists of {len(analogy_df)} analogies\")\n",
    "print(f\"Analogy dataset contains {len(analogy_reduced)} analogies, where all 4 parts are also part of the vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_mul_3_value(source_antecedent, source_conclusion, target_antecedent, target_conclusions, epsilon=0.001):\n",
    "    cos_conclusions = compute_cosine_for_vectors(target_conclusions, source_conclusion, axis_norm_v1=1)\n",
    "    cos_target = compute_cosine_for_vectors(target_conclusions, target_antecedent, axis_norm_v1=1)\n",
    "    nominator = cos_conclusions * cos_target\n",
    "    denominator = compute_cosine_for_vectors(target_conclusions, source_antecedent, axis_norm_v1=1) + epsilon\n",
    "    return nominator / denominator\n",
    "\n",
    "def cos_add_3_value(source_antecedent, source_conclusion, target_antecedent, target_conclusions):\n",
    "    cos_conclusions = compute_cosine_for_vectors(target_conclusions, source_conclusion, axis_norm_v1=1)\n",
    "    cos_source_target = compute_cosine_for_vectors(target_conclusions, source_antecedent, axis_norm_v1=1)\n",
    "    cos_target = compute_cosine_for_vectors(target_conclusions, target_antecedent, axis_norm_v1=1)\n",
    "    return cos_source_target - cos_source_target + cos_target\n",
    "\n",
    "def get_accuracy_cos_3(df_input, we_vectors):\n",
    "    def set_vocab_to_neg_inf(values, words_to_remove, vocab):\n",
    "        for word in words_to_remove:\n",
    "            values[vocab.index(word)] = float('-inf')\n",
    "        return values\n",
    "\n",
    "    def cos_3_predictions(source_antecedent, source_conclusion, target_antecedent,\n",
    "                          vocabulary: list, we_vectors):\n",
    "        if source_antecedent not in vocabulary or \\\n",
    "                source_conclusion not in vocabulary or \\\n",
    "                target_antecedent not in vocabulary:\n",
    "            return np.nan\n",
    "\n",
    "        source_antecedent_vec = get_embedding_from_word(we_vectors, source_antecedent)\n",
    "        source_conclusion_vec = get_embedding_from_word(we_vectors, source_conclusion)\n",
    "        target_antecedent_vec = get_embedding_from_word(we_vectors, target_antecedent)\n",
    "\n",
    "        if np.isnan(source_antecedent_vec).any() or \\\n",
    "                np.isnan(source_conclusion_vec).any() or \\\n",
    "                np.isnan(target_antecedent_vec).any():\n",
    "            return np.nan\n",
    "\n",
    "#        print(f\"Check for '{source_antecedent}' is to '{source_conclusion}' as '{target_antecedent}' is to '?'\")\n",
    "        # calculate cosAdd3 values for the whole vocabulary\n",
    "        cos_add_3 = cos_add_3_value(source_antecedent_vec, \\\n",
    "                                    source_conclusion_vec, \\\n",
    "                                    target_antecedent_vec, \\\n",
    "                                    we_vectors.to_numpy())\n",
    "\n",
    "        # remove values ofthe source_antecedent, source_conclusion and target_antecedent of the analogy\n",
    "        cos_add_3 = set_vocab_to_neg_inf(cos_add_3, \\\n",
    "                                         [source_antecedent, source_conclusion, target_antecedent], \\\n",
    "                                         vocabulary)\n",
    "\n",
    "        # calculate cosMul3 values for the whole vocabulary\n",
    "        cos_mul_3 = cos_mul_3_value(source_antecedent_vec, \\\n",
    "                                    source_conclusion_vec, \\\n",
    "                                    target_antecedent_vec, \\\n",
    "                                    we_vectors.to_numpy())\n",
    "\n",
    "        # remove values ofthe source_antecedent, source_conclusion and target_antecedent of the analogy\n",
    "        cos_mul_3 = set_vocab_to_neg_inf(cos_mul_3, \\\n",
    "                                         [source_antecedent, source_conclusion, target_antecedent], \\\n",
    "                                         vocabulary)\n",
    "\n",
    "        max_index_add = np.argmax(cos_add_3)\n",
    "        max_index_mul = np.argmax(cos_mul_3)\n",
    "#         print(\n",
    "#             f\"Answer Add -> '{source_antecedent}' is to '{source_conclusion}' as '{target_antecedent}' is to '{vocabulary[max_index_add]}'\")\n",
    "#         print(\n",
    "#             f\"Answer Mul -> '{source_antecedent}' is to '{source_conclusion}' as '{target_antecedent}' is to '{vocabulary[max_index_mul]}'\")\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        return vocabulary[max_index_add], vocabulary[max_index_mul]\n",
    "\n",
    "    vocabulary = we_vectors.index.tolist()\n",
    "\n",
    "    cloned_input = df_input.copy()\n",
    "    progress_bar = tqdm(total=len(cloned_input))\n",
    "    cloned_input['prediction'] = cloned_input.apply(lambda row: cos_3_predictions(row[\"source_antecedent\"], \\\n",
    "                                                                                  row[\"source_conclusion\"], \\\n",
    "                                                                                  row[\"target_antecedent\"], \\\n",
    "                                                                                  vocabulary, \\\n",
    "                                                                                  we_vectors), \\\n",
    "                                                    axis=1)\n",
    "\n",
    "    pred_np = np.array(cloned_input['prediction'].to_list())\n",
    "    target_np = cloned_input[\"target_conclusion\"].to_numpy()\n",
    "    pred_add = pred_np[:, 0]\n",
    "    pred_mul = pred_np[:, 1]\n",
    "    return accuracy_score(target_np, pred_add), accuracy_score(target_np, pred_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▊                                                                                 | 1/100 [00:00<01:20,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|█▋                                                                                | 2/100 [00:01<01:20,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|██▍                                                                               | 3/100 [00:02<01:21,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|███▎                                                                              | 4/100 [00:03<01:21,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|████                                                                              | 5/100 [00:04<01:21,  1.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|████▉                                                                             | 6/100 [00:05<01:22,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|█████▋                                                                            | 7/100 [00:06<01:20,  1.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|██████▌                                                                           | 8/100 [00:06<01:19,  1.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|███████▍                                                                          | 9/100 [00:07<01:18,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████████                                                                         | 10/100 [00:08<01:17,  1.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|████████▉                                                                        | 11/100 [00:09<01:17,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█████████▋                                                                       | 12/100 [00:10<01:16,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|██████████▌                                                                      | 13/100 [00:11<01:15,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|███████████▎                                                                     | 14/100 [00:12<01:15,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|████████████▏                                                                    | 15/100 [00:13<01:14,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|████████████▉                                                                    | 16/100 [00:13<01:11,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█████████████▊                                                                   | 17/100 [00:14<01:09,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|██████████████▌                                                                  | 18/100 [00:15<01:08,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|███████████████▍                                                                 | 19/100 [00:16<01:07,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|████████████████▏                                                                | 20/100 [00:17<01:05,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|█████████████████                                                                | 21/100 [00:17<01:04,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|█████████████████▊                                                               | 22/100 [00:18<01:04,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██████████████████▋                                                              | 23/100 [00:19<01:02,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|███████████████████▍                                                             | 24/100 [00:20<01:02,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|████████████████████▎                                                            | 25/100 [00:21<01:01,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|█████████████████████                                                            | 26/100 [00:21<00:59,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|█████████████████████▊                                                           | 27/100 [00:22<00:59,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██████████████████████▋                                                          | 28/100 [00:23<00:58,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|███████████████████████▍                                                         | 29/100 [00:24<00:57,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|████████████████████████▎                                                        | 30/100 [00:25<00:57,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|█████████████████████████                                                        | 31/100 [00:26<00:56,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|█████████████████████████▉                                                       | 32/100 [00:26<00:55,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|██████████████████████████▋                                                      | 33/100 [00:27<00:54,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███████████████████████████▌                                                     | 34/100 [00:28<00:54,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|████████████████████████████▎                                                    | 35/100 [00:29<00:54,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|█████████████████████████████▏                                                   | 36/100 [00:30<00:53,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|█████████████████████████████▉                                                   | 37/100 [00:30<00:51,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|██████████████████████████████▊                                                  | 38/100 [00:31<00:50,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███████████████████████████████▌                                                 | 39/100 [00:32<00:50,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████████████████████████████████▍                                                | 40/100 [00:33<00:52,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|█████████████████████████████████▏                                               | 41/100 [00:34<00:51,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|██████████████████████████████████                                               | 42/100 [00:35<00:51,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|██████████████████████████████████▊                                              | 43/100 [00:36<00:50,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|███████████████████████████████████▋                                             | 44/100 [00:37<00:51,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████████████████████████████████████▍                                            | 45/100 [00:38<00:50,  1.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|█████████████████████████████████████▎                                           | 46/100 [00:39<00:50,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|██████████████████████████████████████                                           | 47/100 [00:40<00:49,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|██████████████████████████████████████▉                                          | 48/100 [00:41<00:48,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|███████████████████████████████████████▋                                         | 49/100 [00:41<00:47,  1.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████████████████████████████████████████▌                                        | 50/100 [00:42<00:45,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████████████████████████████████████████▎                                       | 51/100 [00:43<00:44,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|██████████████████████████████████████████                                       | 52/100 [00:44<00:43,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|██████████████████████████████████████████▉                                      | 53/100 [00:45<00:42,  1.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|███████████████████████████████████████████▋                                     | 54/100 [00:46<00:40,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|████████████████████████████████████████████▌                                    | 55/100 [00:47<00:39,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████████████████████████████████████████████▎                                   | 56/100 [00:48<00:37,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|██████████████████████████████████████████████▏                                  | 57/100 [00:48<00:36,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|██████████████████████████████████████████████▉                                  | 58/100 [00:49<00:35,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|███████████████████████████████████████████████▊                                 | 59/100 [00:50<00:34,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|████████████████████████████████████████████████▌                                | 60/100 [00:51<00:33,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|█████████████████████████████████████████████████▍                               | 61/100 [00:52<00:32,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████████████████████████████████████████████████▏                              | 62/100 [00:52<00:30,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|███████████████████████████████████████████████████                              | 63/100 [00:53<00:30,  1.22it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|███████████████████████████████████████████████████▊                             | 64/100 [00:54<00:29,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|████████████████████████████████████████████████████▋                            | 65/100 [00:55<00:28,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|█████████████████████████████████████████████████████▍                           | 66/100 [00:56<00:27,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████████████████████████████████████████████████████▎                          | 67/100 [00:56<00:26,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|███████████████████████████████████████████████████████                          | 68/100 [00:57<00:25,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|███████████████████████████████████████████████████████▉                         | 69/100 [00:58<00:25,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|████████████████████████████████████████████████████████▋                        | 70/100 [00:59<00:24,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|█████████████████████████████████████████████████████████▌                       | 71/100 [01:00<00:23,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|██████████████████████████████████████████████████████████▎                      | 72/100 [01:01<00:23,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████████████████████████████████████████████████████████▏                     | 73/100 [01:01<00:22,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████████████████████████████████████████████████████████▉                     | 74/100 [01:02<00:21,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|████████████████████████████████████████████████████████████▊                    | 75/100 [01:03<00:20,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|█████████████████████████████████████████████████████████████▌                   | 76/100 [01:04<00:19,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|██████████████████████████████████████████████████████████████▎                  | 77/100 [01:05<00:19,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████████████████████████████████████████████████████████████▏                 | 78/100 [01:06<00:18,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████████████████████████████████████████████████████████████▉                 | 79/100 [01:06<00:17,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████████████████████████████████████████████████████████████▊                | 80/100 [01:07<00:16,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|█████████████████████████████████████████████████████████████████▌               | 81/100 [01:08<00:15,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|██████████████████████████████████████████████████████████████████▍              | 82/100 [01:09<00:14,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|███████████████████████████████████████████████████████████████████▏             | 83/100 [01:10<00:13,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████████████████████████████████████████████████████████████████             | 84/100 [01:10<00:13,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████████████████████████████████████████████████████████████████▊            | 85/100 [01:11<00:12,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|█████████████████████████████████████████████████████████████████████▋           | 86/100 [01:12<00:11,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|██████████████████████████████████████████████████████████████████████▍          | 87/100 [01:13<00:10,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|███████████████████████████████████████████████████████████████████████▎         | 88/100 [01:14<00:09,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████████████████████████████████████████████████████████████████████         | 89/100 [01:15<00:08,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████████████████████████████████████████████████████████████████████▉        | 90/100 [01:15<00:08,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▋       | 91/100 [01:16<00:07,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|██████████████████████████████████████████████████████████████████████████▌      | 92/100 [01:17<00:06,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|███████████████████████████████████████████████████████████████████████████▎     | 93/100 [01:18<00:05,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|████████████████████████████████████████████████████████████████████████████▏    | 94/100 [01:19<00:04,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|████████████████████████████████████████████████████████████████████████████▉    | 95/100 [01:20<00:04,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████████████████████████████████████████████████████████████████████████▊   | 96/100 [01:20<00:03,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|██████████████████████████████████████████████████████████████████████████████▌  | 97/100 [01:21<00:02,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|███████████████████████████████████████████████████████████████████████████████▍ | 98/100 [01:22<00:01,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|████████████████████████████████████████████████████████████████████████████████▏| 99/100 [01:23<00:00,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:24<00:00,  1.21it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of the 3AddCos is 0.24\n",
      "The Accuracy of the 3AddCos is 0.0\n"
     ]
    }
   ],
   "source": [
    "# For test purposes !!!\n",
    "analogy_reduced = analogy_reduced.sample(n=100)\n",
    "acc_cos_3 = get_accuracy_cos_3(analogy_reduced[[\"source_antecedent\", \\\n",
    "                                                \"source_conclusion\", \\\n",
    "                                                \"target_antecedent\", \\\n",
    "                                                \"target_conclusion\"]],\\\n",
    "                               words)\n",
    "print(f\"The Accuracy of the 3AddCos is {acc_cos_3[0]}\")\n",
    "print(f\"The Accuracy of the 3AddCos is {acc_cos_3[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-taskB\"></a><h2 style=\"color:rgb(0,120,170)\">Task B: Document Classification with WE (10 points)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This task follows the same instruction for document classification as provided in Assignment 1. You are indeed free to reuse any part of your code in Assignment 1 for this task. In Assignment 1, the representation of each document was created using a bag of words representation followed by dimensionality reduction. In this task, the document representations are created from the pre-trained word embeddings.\n",
    "\n",
    "**Map word embeddings to dictionary words (2 points).** For every word in the dictionary (as discussed and created in Assignment 1), fetch the corresponding word embedding from the pre-trained model. If no embedding is found, initialize the corresponding word embedding randomly.\n",
    "\n",
    "**Document embedding as the average of word embeddings (5 points).** Using the word embeddings, the representation of each document is defined as the *mean of the vectors of each document's words*. In particular, given the document $d$, consisting of words $\\left[ v_1, v_2, ..., v_{|d|} \\right]$, the document representation $\\mathbf{e}_d$ is defined as:\n",
    "\n",
    "$\\mathbf{e}_d = \\frac{1}{|d|}\\sum_{i=1}^{|d|}{\\mathbf{e}_{v_i}}$\n",
    "\n",
    "where $\\mathbf{e}_{v}$ is the vector of the word $v$, and $|d|$ is the length of the document.\n",
    "\n",
    "**Classification and evaluation (3 points)** Using these new document representations, apply <ins>three classification algorithms</ins> and report the evaluation results (based on accuracy metric) on the test set.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Assignment 1 to get corpus and dictinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from lib import import_datasets, get_n_most_common_tokens, \\\n",
    "preprocess_txt, get_best_lsa_reduced_feature_set, \\\n",
    "clean_text_corpus, compute_sparsity, get_vocab_tokens_from_df, \\\n",
    "latent_semantic_analysis, create_corpus_dict, plot_token_dict, \\\n",
    "vectorize_dataframe, calc_baseline_confusion, get_dummy_baseline, \\\n",
    "get_best_model_featureset, get_model_statistics, plot_features\n",
    "\n",
    "import dill\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = \"small\" # \"medium\"\n",
    "# if already preprocessed then set to false to save time\n",
    "PREPROC = True\n",
    "# if already removed out of vocabulary words then set to false to save time\n",
    "REDUCE_WORDS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, val_df = import_datasets(DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREPROC:\n",
    "    train_df = preprocess_txt(train_df)\n",
    "    test_df = preprocess_txt(test_df)\n",
    "    val_df = preprocess_txt(val_df)\n",
    "\n",
    "    with open(\"train_df_preprocessed.pkl\", \"wb\") as f:\n",
    "        f.write(dill.dumps(train_df))\n",
    "    with open(\"test_df_preprocessed.pkl\", \"wb\") as f:\n",
    "        f.write(dill.dumps(test_df))\n",
    "    with open(\"val_df_preprocessed.pkl\", \"wb\") as f:\n",
    "        f.write(dill.dumps(val_df))\n",
    "else:\n",
    "    with open(\"train_df_preprocessed.pkl\", \"rb\") as f:\n",
    "            train_df = dill.loads(f.read())\n",
    "    with open(\"test_df_preprocessed.pkl\", \"rb\") as f:\n",
    "            test_df = dill.loads(f.read())\n",
    "    with open(\"val_df_preprocessed.pkl\", \"rb\") as f:\n",
    "            val_df = dill.loads(f.read())\n",
    "\n",
    "            \n",
    "full_corpus_dictionary = create_corpus_dict(train_df[\"text\"])\n",
    "reduced_corpus_dictionary = create_corpus_dict(train_df[\"text\"], \\\n",
    "                                            threshold=150)\n",
    "reduced_corpus_dictionary.drop(\"count\", axis=1)\n",
    "    \n",
    "if REDUCE_WORDS:\n",
    "    \n",
    "    threshold = 150\n",
    "    train_df = clean_text_corpus(train_df, \\\n",
    "                                 full_corpus_dictionary, \\\n",
    "                                 threshold=threshold)\n",
    "    test_df = clean_text_corpus(test_df, \\\n",
    "                                full_corpus_dictionary, \\\n",
    "                                threshold=threshold)\n",
    "    val_df = clean_text_corpus(val_df, \\\n",
    "                               full_corpus_dictionary, \\\n",
    "                               threshold=threshold)\n",
    "\n",
    "    with open(\"train_df_reduced.pkl\", \"wb\") as f:\n",
    "        f.write(dill.dumps(train_df))\n",
    "    with open(\"test_df_reduced.pkl\", \"wb\") as f:\n",
    "        f.write(dill.dumps(test_df))\n",
    "    with open(\"val_df_reduced.pkl\", \"wb\") as f:\n",
    "        f.write(dill.dumps(val_df))\n",
    "\n",
    "else:\n",
    "    with open(\"train_df_reduced.pkl\", \"rb\") as f:\n",
    "            train_df = dill.loads(f.read())\n",
    "    with open(\"test_df_reduced.pkl\", \"rb\") as f:\n",
    "            test_df = dill.loads(f.read())\n",
    "    with open(\"val_df_reduced.pkl\", \"rb\") as f:\n",
    "            val_df = dill.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dictionary based on the Glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding_for_document_embedding(embeddings_df, word):\n",
    "    \"\"\" \n",
    "        takes a premade word embedding and a word\n",
    "        and returns the respective embedding or a \n",
    "        random vector if the word is not found in\n",
    "        the embedding\n",
    "    \"\"\"\n",
    "    embedding = get_embedding_from_word(embeddings_df, word)\n",
    "\n",
    "    if type(embedding) is float:\n",
    "        embedding = np.random.rand(*get_embedding_matrix(embeddings_df)[0].shape, ) * 2 - 1\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new DataFrame for storing the pre embedded dictionary\n",
    "embeddings_df = pd.DataFrame()\n",
    "\n",
    "for token in reduced_corpus_dictionary.loc[:, \"token\"]:\n",
    "    embeddings_df = embeddings_df.append([[token, *get_word_embedding_for_document_embedding(words, token)]])\n",
    "    \n",
    "embeddings_df.columns = [\"token\", *range(1, len(words.iloc[0]) + 1)]\n",
    "embeddings_df = embeddings_df.set_index(\"token\")\n",
    "embeddings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Document Embeddings based on the Glove Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(word_embeddings, document_words):\n",
    "    \"\"\"\n",
    "        computes the embedding of a document based on the \n",
    "        description in task 2\n",
    "    \"\"\"\n",
    "    d = len(document_words)\n",
    "    document_vec = np.zeros_like(get_embedding_matrix(word_embeddings)[0], dtype=np.float64)\n",
    "    \n",
    "    for dw in document_words:\n",
    "        document_vec = document_vec + get_embedding_from_word(word_embeddings, dw)\n",
    "    \n",
    "    return document_vec / d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embeddings_df(words_df, documents_df):\n",
    "    \"\"\"\n",
    "        takes a DataFrame of word embeddings and another of\n",
    "        documents to output a DataFrame of document embeddings\n",
    "        based on the description in task 2\n",
    "    \"\"\"\n",
    "    \n",
    "    document_embeddings_lst = []\n",
    "    for doc_idx in range(len(documents_df)):\n",
    "        document_embeddings_lst.append(get_document_embedding(words_df, documents_df.iloc[doc_idx][\"text\"].split(\" \")))\n",
    "\n",
    "    ret_df = pd.DataFrame(document_embeddings_lst)\n",
    "    ret_df.index = documents_df.index\n",
    "    return ret_df\n",
    "    \n",
    "document_embedding = get_document_embeddings_df(embeddings_df, train_df[:10])\n",
    "\n",
    "# this is the document embedding based on the glove pre embedded tokens\n",
    "document_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-taskC\"></a><h2 style=\"color:rgb(0,120,170)\">Task C: Classification with sent2vec Document Embeddings (2 extra point)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Sent2vec [2] suggests another unsupervised approach to creating document embeddings from the underlying word embeddings. First, using the provided code in the paper, train a sendtvec model on the training set to create document embeddings. Then, repeat Task B while using the document embeddings provided by sent2vec. Similar to Task 2, conduct the classification experiments and report evaluation results.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-references\"></a><h2 style=\"color:rgb(0,120,170)\">References</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] O. Levy, Y. Goldberg, and I. Dagan. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211– 225, 2015.\n",
    "\n",
    "[2] M. Pagliardini, P. Gupta, and M. Jaggi. Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features. In Proceedings of the conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

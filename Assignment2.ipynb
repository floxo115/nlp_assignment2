{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill out the information of your group!\n",
    "\n",
    "| <p style=\"text-align: center;\">First Name</p>  | <p style=\"text-align: center;\">Family Name</p> | Matr.-No. |\n",
    "| ---------------------------------------------- | ---------------------------------------------- | -------- |\n",
    "| <p style=\"text-align: left\">*EDIT!*</p>| <p style=\"text-align: left\">*EDIT!*</p> | *EDIT!* |\n",
    "| <p style=\"text-align: left\">Michael</p>| <p style=\"text-align: left\">Weikl</p> | k01154652 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center\">344.105/6/7 UE: Natural Language Processing (WS2021/22)</h2>\n",
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 2</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Getting to Know Word Embedding!</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Terms of Use</b><br>\n",
    "This  material is prepared for educational purposes at the Johannes Kepler University Linz (JKU), and is exclusively provided to the registered students of the mentioned course at JKU. It is strictly forbidden to distribute the current file, the contents of the assignment, and its solution. The use or reproduction of this manuscript is only allowed for educational purposes in non-profit organizations, while in this case, the explicit prior acceptance of the author(s) is required.\n",
    "\n",
    "**Author:** Navid Rekab-saz<br>\n",
    "**Email:** navid.rekabsaz@jku.at<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of contents</h2>\n",
    "<ol>\n",
    "    <a href=\"#section-general-guidelines\"><li style=\"font-size:large;font-weight:bold\">General Guidelines</li></a>\n",
    "    <a href=\"#section-taskA\"><li style=\"font-size:large;font-weight:bold\">Task A: Similarity, Nearest Neighbors, and WE Evaluation (20 points)</li></a>\n",
    "    <a href=\"#section-taskB\"><li style=\"font-size:large;font-weight:bold\">Task B: Document Classification with WE (10 points)</li></a>\n",
    "    <a href=\"#section-taskC\"><li style=\"font-size:large;font-weight:bold\">Task C: Classification with sent2vec Document Embeddings (2 extra point)</li></a>\n",
    "    <a href=\"#section-references\"><li style=\"font-size:large;font-weight:bold\">References</li></a>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-general-guidelines\"></a><h2 style=\"color:rgb(0,120,170)\">General Guidelines</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Assignment objective\n",
    "The aim of this assignment is to get familiarized with using word embedding (WE) models in practice. The assignment in total has **30 points**; it also offers **2 extra points** which can cover any missing point.\n",
    "\n",
    "This Notebook encompasses all aspects of the assignment, namely the descriptions of tasks as well as your solutions and reports. Feel free to add any required cell for solutions. The cells can contain code, reports, charts, tables, or any other material, required for the assignment. Feel free to provide the solutions in an interactive and visual way! \n",
    "\n",
    "Please discuss any unclear point in the assignment in the provided forum in MOODLE. It is also encouraged to provide answers to your peer's questions. However when submitting a post, keep in mind to avoid providing solutions. Please let the tutor(s) know shall you find any error or unclarity in the assignment.\n",
    "\n",
    "### Implementation & Libraries\n",
    "\n",
    "The assignment should be implemented with recent versions of `Python` (>3.7). Any standard Python library can be used, so far that the library is free and can be simply installed using `pip` or `conda`. Examples of potentially useful libraries are `scikit-learn`, `numpy`, `scipy`, `gensim`, `nltk`, `spaCy`, and `AllenNLP`. Use the latest stable version of each library.\n",
    "\n",
    "### Submission\n",
    "\n",
    "Each group submits one Notebook file (`.ipynb`) through MOODLE. Do not forget to put in your names and student numbers in the first cell of the Notebook. **In the submitted Notebook, all the results and visualizations should already be present, and can be observed simply by loading the Notebook in a browser.** The Notebook must be self-contained, meaning that one can run all the cells from top to bottom without any error. If you need to include extra files in the submission, compress all files (together with the Notebook) in a `zip` file and submit the zip file to MOODLE. You do not need to include the data files in the submission.\n",
    "\n",
    "Cover the questions/points, mentioned in the tasks, but also add any necessary point for understanding your experiments.  \n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "To conduct the experiments, two datasets are provided. The datasets are taken from the data of `thedeep` project, produced by the DEEP (https://www.thedeep.io) platform. The DEEP is an open-source platform, which aims to facilitate processing of textual data for international humanitarian response organizations. The platform enables the classification of text excerpts, extracted from news and reports into a set of domain specific classes. The provided dataset has 12 classes (labels) like agriculture, health, and protection. \n",
    "\n",
    "The difference between the datasets is in their sizes. We refer to these as `medium` and `small`, containing an overall number of 38,000 and 12,000 annotated text excerpts, respectively. Select one of the datasets, and use it for all of the tasks. `medium` provides more data and therefore reflects a more realistic scenario. `small` is however provided for the sake of convenience, particularly if running the experiments on your available hardware takes too long. Using `medium` is generally recommended, but from the point of view of assignment grading, there is no difference between the datasets.\n",
    "\n",
    "Download the dataset from [this link](https://drive.jku.at/filr/public-link/file-download/0cce88f07c9c862b017c9cfba294077a/33590/5792942781153185740/nlp2021_22_data.zip).\n",
    "\n",
    "Whether `medium` or `small`, you will find the following files in the provided zip file:\n",
    "- `thedeep.$name$.train.txt`: Train set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.validation.txt`: Validation set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.test.txt`: Test set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.label.txt`: Captions of the labels.\n",
    "- `README.txt`: Terms of use of the dataset.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-taskA\"></a><h2 style=\"color:rgb(0,120,170)\">Task A: Similarity, Nearest Neighbors, and WE Evaluation (20 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Loading a word embedding (WE) model (2 points).** Download a pre-trained word embedding model such as word2vec (https://code.google.com/archive/p/word2vec/) or GloVe (https://nlp.stanford.edu/projects/glove/). You can load the downloaded vectors into arrays, or use libraries such as `gensim` to download and process the vectors. \n",
    "\n",
    "**Calculating word-to-word similarities (3 points).** Select <ins>3 arbitrary words</ins>, referred to as *source words*. For each source word, calculate the cosine similarities to the <ins>5 other words</ins>, that from your own linguistic judgement have various levels of semantic relations to the source word (from highly related to not related at all). Compare the output similarities with your judgements on the semantic relations of words, and report your observations. Consider that when calculating the cosine similarities, DO NOT use the provided functionalities of any library, but implement the cosine similarity as a function that takes two vectors and returns a similarity score.\n",
    "\n",
    "**Calculating nearest neighbors (7 points).** For the selected source words, retrieve the 10 nearest neighbors in the word embedding model, namely the words that have the highest similarities to the source word. Consider that when calculating the nearest neighbors, DO NOT use the provided functionalities of any library but implement it as the function that takes a source vector, a set of target vectors, and a $k$ parameter, and returns the $k$ nearest neighbors and their similarity scores. This function should provide an *efficient* calculation of nearest neighbors. An inefficient way (which should be avoided!) would be looping over the set of vectors in the word embedding model, and one by one calculating the cosine similarity of a source vector to each of the target vectors. As a hint for an efficient way, consider that in `numpy` (and other libraries), calculating the dot product of a vector to a matrix is much faster than the dot products of the vector to each vector of the matrix. (**3 out of 7 points for efficiency**)\n",
    "\n",
    "**WE evaluation (8 points).** There have been several efforts in devising benchmarking datasets to intrinsically evaluate the *goodness* of word embedding models. [Levy et al.](https://aclanthology.org/Q15-1016.pdf) [1] in Section 4.3 describe the two evaluation category of *Word Similarity* and *Analogy* tasks, reference the various datasets of each task, explain the method(s) to calculate the required quantities, and finally discribes the method to evaluate the models on the tasks. Following the descriptions in the paper, select one word similarity and one analogy dataset, download the corresponding datasets (the datasets are publicly available –– they can also be found through the cited papers), and conduct the evaluation process on the WE model. For the analogy task, conduct the experiments using both `3CosAdd` and `3CosMul` methods. Report the evaluation results. Your results should be in the range of the ones reported in the paper (for sanity check). As before, provide your own implementation of the calculations and DO NOT use the functions of libraries.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from os import system\n",
    "import requests\n",
    "import zipfile, tarfile\n",
    "import io\n",
    "import pathlib\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import rankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_FILE = \"glove/glove.6B.50d.txt\"\n",
    "WORDSIM_FILE = \"wordsim/wordsim_similarity_goldstandard.txt\"\n",
    "ANALOGY_FILE = \"analogy/word-test.v1.txt\"\n",
    "\n",
    "DOWNLOAD_GLOVE = False\n",
    "# DOWNLOAD_WORDSIM = True\n",
    "# DOWNLOAD_ANALOGY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, fname: str, stream=True, unit_divisor=1024):\n",
    "    resp = requests.get(url, stream=stream)\n",
    "    \n",
    "    if stream:\n",
    "        total = int(resp.headers.get('content-length', 0))\n",
    "        print(total)\n",
    "        with open(fname, 'wb') as file, tqdm(\\\n",
    "            desc=fname,\\\n",
    "            total=total,\\\n",
    "            unit='iB',\\\n",
    "            unit_scale=True,\\\n",
    "            unit_divisor=unit_divisor,\\\n",
    "        ) as bar:\n",
    "            for data in resp.iter_content(chunk_size=unit_divisor):\n",
    "                size = file.write(data)\n",
    "                bar.update(size)\n",
    "    else:\n",
    "        with open(fname, \"wb\") as file:\n",
    "            file.write(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_GLOVE:\n",
    "    glove_dir = pathlib.Path(\"./glove\")\n",
    "    if glove_dir.exists():\n",
    "        shutil.rmtree(glove_dir)\n",
    "    glove_dir.mkdir(parents=False, exist_ok=False)\n",
    "    \n",
    "    zip_path = \"./glove/glove.zip\"\n",
    "    download(url=\"https://nlp.stanford.edu/data/glove.6B.zip\", fname=zip_path)\n",
    "    zip_file = zipfile.ZipFile(zip_path, 'r')\n",
    "    zip_file.extractall(glove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.249680</td>\n",
       "      <td>-0.41242</td>\n",
       "      <td>0.121700</td>\n",
       "      <td>0.34527</td>\n",
       "      <td>-0.044457</td>\n",
       "      <td>-0.496880</td>\n",
       "      <td>-0.178620</td>\n",
       "      <td>-0.000660</td>\n",
       "      <td>-0.656600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298710</td>\n",
       "      <td>-0.157490</td>\n",
       "      <td>-0.347580</td>\n",
       "      <td>-0.045637</td>\n",
       "      <td>-0.442510</td>\n",
       "      <td>0.187850</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>-0.184110</td>\n",
       "      <td>-0.115140</td>\n",
       "      <td>-0.785810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.013441</td>\n",
       "      <td>0.236820</td>\n",
       "      <td>-0.16899</td>\n",
       "      <td>0.409510</td>\n",
       "      <td>0.63812</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>-0.428520</td>\n",
       "      <td>-0.556410</td>\n",
       "      <td>-0.364000</td>\n",
       "      <td>-0.239380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080262</td>\n",
       "      <td>0.630030</td>\n",
       "      <td>0.321110</td>\n",
       "      <td>-0.467650</td>\n",
       "      <td>0.227860</td>\n",
       "      <td>0.360340</td>\n",
       "      <td>-0.378180</td>\n",
       "      <td>-0.566570</td>\n",
       "      <td>0.044691</td>\n",
       "      <td>0.303920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.151640</td>\n",
       "      <td>0.301770</td>\n",
       "      <td>-0.16763</td>\n",
       "      <td>0.176840</td>\n",
       "      <td>0.31719</td>\n",
       "      <td>0.339730</td>\n",
       "      <td>-0.434780</td>\n",
       "      <td>-0.310860</td>\n",
       "      <td>-0.449990</td>\n",
       "      <td>-0.294860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.068987</td>\n",
       "      <td>0.087939</td>\n",
       "      <td>-0.102850</td>\n",
       "      <td>-0.139310</td>\n",
       "      <td>0.223140</td>\n",
       "      <td>-0.080803</td>\n",
       "      <td>-0.356520</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.102160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.708530</td>\n",
       "      <td>0.570880</td>\n",
       "      <td>-0.47160</td>\n",
       "      <td>0.180480</td>\n",
       "      <td>0.54449</td>\n",
       "      <td>0.726030</td>\n",
       "      <td>0.181570</td>\n",
       "      <td>-0.523930</td>\n",
       "      <td>0.103810</td>\n",
       "      <td>-0.175660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.347270</td>\n",
       "      <td>0.284830</td>\n",
       "      <td>0.075693</td>\n",
       "      <td>-0.062178</td>\n",
       "      <td>-0.389880</td>\n",
       "      <td>0.229020</td>\n",
       "      <td>-0.216170</td>\n",
       "      <td>-0.225620</td>\n",
       "      <td>-0.093918</td>\n",
       "      <td>-0.803750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.680470</td>\n",
       "      <td>-0.039263</td>\n",
       "      <td>0.30186</td>\n",
       "      <td>-0.177920</td>\n",
       "      <td>0.42962</td>\n",
       "      <td>0.032246</td>\n",
       "      <td>-0.413760</td>\n",
       "      <td>0.132280</td>\n",
       "      <td>-0.298470</td>\n",
       "      <td>-0.085253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094375</td>\n",
       "      <td>0.018324</td>\n",
       "      <td>0.210480</td>\n",
       "      <td>-0.030880</td>\n",
       "      <td>-0.197220</td>\n",
       "      <td>0.082279</td>\n",
       "      <td>-0.094340</td>\n",
       "      <td>-0.073297</td>\n",
       "      <td>-0.064699</td>\n",
       "      <td>-0.260440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chanty</th>\n",
       "      <td>0.232040</td>\n",
       "      <td>0.025672</td>\n",
       "      <td>-0.70699</td>\n",
       "      <td>-0.045465</td>\n",
       "      <td>0.13989</td>\n",
       "      <td>-0.628070</td>\n",
       "      <td>0.726250</td>\n",
       "      <td>0.341080</td>\n",
       "      <td>0.446140</td>\n",
       "      <td>0.163290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095526</td>\n",
       "      <td>-0.296050</td>\n",
       "      <td>0.385670</td>\n",
       "      <td>0.136840</td>\n",
       "      <td>0.593310</td>\n",
       "      <td>-0.694860</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>-0.180690</td>\n",
       "      <td>-0.258300</td>\n",
       "      <td>-0.039673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kronik</th>\n",
       "      <td>-0.609210</td>\n",
       "      <td>-0.672180</td>\n",
       "      <td>0.23521</td>\n",
       "      <td>-0.111950</td>\n",
       "      <td>-0.46094</td>\n",
       "      <td>-0.007462</td>\n",
       "      <td>0.255780</td>\n",
       "      <td>0.856320</td>\n",
       "      <td>0.055977</td>\n",
       "      <td>-0.237920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672050</td>\n",
       "      <td>-0.598220</td>\n",
       "      <td>-0.202590</td>\n",
       "      <td>0.392430</td>\n",
       "      <td>0.028873</td>\n",
       "      <td>0.030003</td>\n",
       "      <td>-0.106170</td>\n",
       "      <td>-0.114110</td>\n",
       "      <td>-0.249010</td>\n",
       "      <td>-0.120260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rolonda</th>\n",
       "      <td>-0.511810</td>\n",
       "      <td>0.058706</td>\n",
       "      <td>1.09130</td>\n",
       "      <td>-0.551630</td>\n",
       "      <td>-0.10249</td>\n",
       "      <td>-0.126500</td>\n",
       "      <td>0.995030</td>\n",
       "      <td>0.079711</td>\n",
       "      <td>-0.162460</td>\n",
       "      <td>0.564880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024747</td>\n",
       "      <td>0.200920</td>\n",
       "      <td>-1.085100</td>\n",
       "      <td>-0.136260</td>\n",
       "      <td>0.350520</td>\n",
       "      <td>-0.858910</td>\n",
       "      <td>0.067858</td>\n",
       "      <td>-0.250030</td>\n",
       "      <td>-1.125000</td>\n",
       "      <td>1.586300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsombor</th>\n",
       "      <td>-0.758980</td>\n",
       "      <td>-0.474260</td>\n",
       "      <td>0.47370</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>-0.78064</td>\n",
       "      <td>0.232330</td>\n",
       "      <td>0.046114</td>\n",
       "      <td>0.840140</td>\n",
       "      <td>0.243710</td>\n",
       "      <td>0.022978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454390</td>\n",
       "      <td>-0.842540</td>\n",
       "      <td>0.106500</td>\n",
       "      <td>-0.059397</td>\n",
       "      <td>0.090449</td>\n",
       "      <td>0.305810</td>\n",
       "      <td>-0.614240</td>\n",
       "      <td>0.789540</td>\n",
       "      <td>-0.014116</td>\n",
       "      <td>0.644800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sandberger</th>\n",
       "      <td>0.072617</td>\n",
       "      <td>-0.513930</td>\n",
       "      <td>0.47280</td>\n",
       "      <td>-0.522020</td>\n",
       "      <td>-0.35534</td>\n",
       "      <td>0.346290</td>\n",
       "      <td>0.232110</td>\n",
       "      <td>0.230960</td>\n",
       "      <td>0.266940</td>\n",
       "      <td>0.410280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688800</td>\n",
       "      <td>-0.179860</td>\n",
       "      <td>-0.066569</td>\n",
       "      <td>-0.480440</td>\n",
       "      <td>-0.559460</td>\n",
       "      <td>-0.275940</td>\n",
       "      <td>0.056072</td>\n",
       "      <td>-0.189070</td>\n",
       "      <td>-0.590210</td>\n",
       "      <td>0.555590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1         2        3         4        5         6   \\\n",
       "word                                                                   \n",
       "the         0.418000  0.249680 -0.41242  0.121700  0.34527 -0.044457   \n",
       ",           0.013441  0.236820 -0.16899  0.409510  0.63812  0.477090   \n",
       ".           0.151640  0.301770 -0.16763  0.176840  0.31719  0.339730   \n",
       "of          0.708530  0.570880 -0.47160  0.180480  0.54449  0.726030   \n",
       "to          0.680470 -0.039263  0.30186 -0.177920  0.42962  0.032246   \n",
       "...              ...       ...      ...       ...      ...       ...   \n",
       "chanty      0.232040  0.025672 -0.70699 -0.045465  0.13989 -0.628070   \n",
       "kronik     -0.609210 -0.672180  0.23521 -0.111950 -0.46094 -0.007462   \n",
       "rolonda    -0.511810  0.058706  1.09130 -0.551630 -0.10249 -0.126500   \n",
       "zsombor    -0.758980 -0.474260  0.47370  0.772500 -0.78064  0.232330   \n",
       "sandberger  0.072617 -0.513930  0.47280 -0.522020 -0.35534  0.346290   \n",
       "\n",
       "                  7         8         9         10  ...        41        42  \\\n",
       "word                                                ...                       \n",
       "the        -0.496880 -0.178620 -0.000660 -0.656600  ... -0.298710 -0.157490   \n",
       ",          -0.428520 -0.556410 -0.364000 -0.239380  ... -0.080262  0.630030   \n",
       ".          -0.434780 -0.310860 -0.449990 -0.294860  ... -0.000064  0.068987   \n",
       "of          0.181570 -0.523930  0.103810 -0.175660  ... -0.347270  0.284830   \n",
       "to         -0.413760  0.132280 -0.298470 -0.085253  ... -0.094375  0.018324   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "chanty      0.726250  0.341080  0.446140  0.163290  ... -0.095526 -0.296050   \n",
       "kronik      0.255780  0.856320  0.055977 -0.237920  ...  0.672050 -0.598220   \n",
       "rolonda     0.995030  0.079711 -0.162460  0.564880  ...  0.024747  0.200920   \n",
       "zsombor     0.046114  0.840140  0.243710  0.022978  ...  0.454390 -0.842540   \n",
       "sandberger  0.232110  0.230960  0.266940  0.410280  ...  0.688800 -0.179860   \n",
       "\n",
       "                  43        44        45        46        47        48  \\\n",
       "word                                                                     \n",
       "the        -0.347580 -0.045637 -0.442510  0.187850  0.002785 -0.184110   \n",
       ",           0.321110 -0.467650  0.227860  0.360340 -0.378180 -0.566570   \n",
       ".           0.087939 -0.102850 -0.139310  0.223140 -0.080803 -0.356520   \n",
       "of          0.075693 -0.062178 -0.389880  0.229020 -0.216170 -0.225620   \n",
       "to          0.210480 -0.030880 -0.197220  0.082279 -0.094340 -0.073297   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "chanty      0.385670  0.136840  0.593310 -0.694860  0.124100 -0.180690   \n",
       "kronik     -0.202590  0.392430  0.028873  0.030003 -0.106170 -0.114110   \n",
       "rolonda    -1.085100 -0.136260  0.350520 -0.858910  0.067858 -0.250030   \n",
       "zsombor     0.106500 -0.059397  0.090449  0.305810 -0.614240  0.789540   \n",
       "sandberger -0.066569 -0.480440 -0.559460 -0.275940  0.056072 -0.189070   \n",
       "\n",
       "                  49        50  \n",
       "word                            \n",
       "the        -0.115140 -0.785810  \n",
       ",           0.044691  0.303920  \n",
       ".           0.016413  0.102160  \n",
       "of         -0.093918 -0.803750  \n",
       "to         -0.064699 -0.260440  \n",
       "...              ...       ...  \n",
       "chanty     -0.258300 -0.039673  \n",
       "kronik     -0.249010 -0.120260  \n",
       "rolonda    -1.125000  1.586300  \n",
       "zsombor    -0.014116  0.644800  \n",
       "sandberger -0.590210  0.555590  \n",
       "\n",
       "[400000 rows x 50 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pd.read_table(VECTOR_FILE, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "words.index.name = \"word\"\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_from_word(dataframe, word):\n",
    "    try:\n",
    "        result = dataframe.loc[word, 1:].to_numpy()\n",
    "    except KeyError:\n",
    "        result = np.nan\n",
    "        \n",
    "    return result\n",
    "\n",
    "def get_embedding_matrix(dataframe):\n",
    "    return dataframe.to_numpy()\n",
    "\n",
    "def compute_cosine_for_vectors(vector_1, vector_2):\n",
    "    return (vector_1 @ vector_2)/(np.linalg.norm(vector_1) * np.linalg.norm(vector_2))\n",
    "\n",
    "def compute_cosine_for_words(word1, word2, words):\n",
    "    word_vector_1 = get_embedding_from_word(words, word1)\n",
    "    word_vector_2 = get_embedding_from_word(words, word2)\n",
    "    \n",
    "    if np.isnan(word_vector_1).any() or np.isnan(word_vector_2).any():\n",
    "        return np.nan\n",
    "    \n",
    "    return compute_cosine_for_vectors(word_vector_1, word_vector_2)\n",
    "\n",
    "def compare_cosine_for_base(base_word, comparisson_words, words):\n",
    "    for test_word in comparisson_words:\n",
    "        print(f\"cosine similarity of {base_word} with {test_word} is \"\n",
    "              f\"{round(compute_cosine_for_words(base_word, test_word, words),4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of train with station is 0.7299\n",
      "cosine similarity of train with schedule is 0.4354\n",
      "cosine similarity of train with plane is 0.6904\n",
      "cosine similarity of train with mouse is 0.2314\n",
      "cosine similarity of train with tree is 0.2624\n"
     ]
    }
   ],
   "source": [
    "source_word_1 = \"train\"\n",
    "source_word_1_tests = [\"station\",\"schedule\", \"plane\", \"mouse\", \"tree\"]\n",
    "\n",
    "compare_cosine_for_base(source_word_1, source_word_1_tests, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of sun with star is 0.4563\n",
      "cosine similarity of sun with light is 0.6165\n",
      "cosine similarity of sun with ray is 0.4723\n",
      "cosine similarity of sun with cold is 0.5325\n",
      "cosine similarity of sun with car is 0.2324\n"
     ]
    }
   ],
   "source": [
    "source_word_2 = \"sun\"\n",
    "source_word_2_tests = [\"star\",\"light\", \"ray\", \"cold\", \"car\"]\n",
    "\n",
    "compare_cosine_for_base(source_word_2, source_word_2_tests, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of calculator with calculation is 0.6064\n",
      "cosine similarity of calculator with computer is 0.5805\n",
      "cosine similarity of calculator with symbols is 0.1603\n",
      "cosine similarity of calculator with speed is 0.3166\n",
      "cosine similarity of calculator with whale is 0.074\n"
     ]
    }
   ],
   "source": [
    "source_word_3 = \"calculator\"\n",
    "source_word_3_tests = [\"calculation\",\"computer\", \"symbols\", \"speed\", \"whale\"]\n",
    "\n",
    "compare_cosine_for_base(source_word_3, source_word_3_tests, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_nearest_neigbours(source_vector, words, k): \n",
    "    B = get_embedding_matrix(words)\n",
    "    \n",
    "    distances = np.linalg.norm(B - source_vector, axis=1)\n",
    "    cos = (B @ source_vector)/(np.linalg.norm(B, axis=1)*np.linalg.norm(source_vector))\n",
    "    nearest_idx = np.argsort(distances)[:k]\n",
    "   \n",
    "    results = []\n",
    "    for i in nearest_idx:\n",
    "        results.append((words.index[i], round(cos[i],4)))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('train', 1.0),\n",
       " ('bus', 0.9016),\n",
       " ('trains', 0.8935),\n",
       " ('traveling', 0.8003),\n",
       " ('travelling', 0.7892),\n",
       " ('buses', 0.8184),\n",
       " ('taxi', 0.7694),\n",
       " ('stops', 0.7569),\n",
       " ('passenger', 0.798),\n",
       " ('ferry', 0.7534)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_nearest_neigbours(get_embedding_from_word(words, source_word_1), words, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sun', 1.0),\n",
       " ('sky', 0.6637),\n",
       " ('cloud', 0.6327),\n",
       " ('moon', 0.6543),\n",
       " ('bright', 0.6353),\n",
       " ('sunshine', 0.5972),\n",
       " ('hung', 0.6211),\n",
       " ('light', 0.6165),\n",
       " ('blue', 0.6263),\n",
       " ('o', 0.5909)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_nearest_neigbours(get_embedding_from_word(words, source_word_2), words, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('calculator', 1.0),\n",
       " ('calculators', 0.8655),\n",
       " ('graphing', 0.7849),\n",
       " ('calculates', 0.642),\n",
       " ('gadget', 0.6686),\n",
       " ('printout', 0.6422),\n",
       " ('specs', 0.6184),\n",
       " ('saver', 0.6331),\n",
       " ('computerized', 0.6741),\n",
       " ('pda', 0.6307)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_k_nearest_neigbours(get_embedding_from_word(words, source_word_3), words, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if DOWNLOAD_WORDSIM:\n",
    "#     wordsim_dir = pathlib.Path(\"./wordsim\")\n",
    "#     print(wordsim_dir.exists())\n",
    "#     if wordsim_dir.exists():\n",
    "#         shutil.rmtree(wordsim_dir)\n",
    "#     wordsim_dir.mkdir(parents=False, exist_ok=False)\n",
    "    \n",
    "#     tar_path = \"./wordsim/ws353simrel.tar.gz\"\n",
    "#     download(url=\"http://alfonseca.org/pubs/ws353simrel.tar.gz\", fname=tar_path, stream=False)\n",
    "#     shutil.unpack_archive(tar_path, wordsim_dir)\n",
    "# #     tar_file = tarfile.open(tar_path, 'r:gz')\n",
    "# #     tar_file.extractall(wordsim_dir)\n",
    "\n",
    "# if DOWNLOAD_ANALOGY:\n",
    "#     analogy_dir = pathlib.Path(\"./analogy\")\n",
    "#     if analogy_dir.exists():\n",
    "#         shutil.rmtree(analogy_dir)\n",
    "#     analogy_dir.mkdir(parents=False, exist_ok=False)\n",
    "    \n",
    "#     txt_path = \"./analogy/word-test.v1.txt\"\n",
    "#     download(url=\"http://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt\", fname=txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_array(array):\n",
    "    print(array)\n",
    "    tmp = array.argsort()\n",
    "    print(tmp)\n",
    "    rank = tmp.argsort()\n",
    "    print(rank)\n",
    "    rank += 1\n",
    "    print(rank)\n",
    "    \n",
    "    r1 = rankdata(array)\n",
    "    print(r1)\n",
    "        \n",
    "    \n",
    "    return rank\n",
    "    \n",
    "    \n",
    "def spearman_correlation(x: np.ndarray, y: np.ndarray):\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(f\"Length of given arrays do not match. x: '{len(x)}', y: '{len(y)}'\")\n",
    "    \n",
    "    x_r = rankdata(x)\n",
    "    y_r = rankdata(y)\n",
    "    \n",
    "    x_r2 = x_r**2\n",
    "    y_r2 = y_r**2\n",
    "    \n",
    "    n = len(x)\n",
    "    prod_r1_r2 = x_r * y_r\n",
    "    \n",
    "    correlation = (n * (np.sum(prod_r1_r2))) - (np.sum(x_r) * np.sum(y_r))\n",
    "    std_x_r = np.sqrt((n * (np.sum(x_r2))) - np.sum(x_r)**2)\n",
    "    std_y_r = np.sqrt((n * (np.sum(y_r2))) - np.sum(y_r)**2)\n",
    "    \n",
    "    spearman = correlation / (std_x_r * std_y_r)\n",
    "    return spearman\n",
    "\n",
    "def cos_add_3():\n",
    "    pass\n",
    "\n",
    "def cos_mul_3():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word similarity via the WordSim353 - Similarity is: 0.57084\n"
     ]
    }
   ],
   "source": [
    "wordsim_df = pd.read_csv(WORDSIM_FILE, sep='\\t',\n",
    "                         names=[\"word1\", \"word2\", \"rating\"])\n",
    "\n",
    "wordsim_df['cosine_sim'] = wordsim_df.apply(lambda row: compute_cosine_for_words(row['word1'], \\\n",
    "                                                                                 row['word2'], \\\n",
    "                                                                                 words), \\\n",
    "                                            axis = 1)\n",
    "\n",
    "wordsim_df.dropna(subset = [\"cosine_sim\"], inplace=True)\n",
    "sc = spearman_correlation(wordsim_df['cosine_sim'].to_numpy(), wordsim_df['rating'].to_numpy())\n",
    "sc1 = wordsim_df[['rating', 'cosine_sim']].corr(method=\"spearman\")\n",
    "\n",
    "print(f\"The word similarity via the WordSim353 - Similarity is: {sc:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-taskB\"></a><h2 style=\"color:rgb(0,120,170)\">Task B: Document Classification with WE (10 points)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This task follows the same instruction for document classification as provided in Assignment 1. You are indeed free to reuse any part of your code in Assignment 1 for this task. In Assignment 1, the representation of each document was created using a bag of words representation followed by dimensionality reduction. In this task, the document representations are created from the pre-trained word embeddings.\n",
    "\n",
    "**Map word embeddings to dictionary words (2 points).** For every word in the dictionary (as discussed and created in Assignment 1), fetch the corresponding word embedding from the pre-trained model. If no embedding is found, initialize the corresponding word embedding randomly.\n",
    "\n",
    "**Document embedding as the average of word embeddings (5 points).** Using the word embeddings, the representation of each document is defined as the *mean of the vectors of each document's words*. In particular, given the document $d$, consisting of words $\\left[ v_1, v_2, ..., v_{|d|} \\right]$, the document representation $\\mathbf{e}_d$ is defined as:\n",
    "\n",
    "$\\mathbf{e}_d = \\frac{1}{|d|}\\sum_{i=1}^{|d|}{\\mathbf{e}_{v_i}}$\n",
    "\n",
    "where $\\mathbf{e}_{v}$ is the vector of the word $v$, and $|d|$ is the length of the document.\n",
    "\n",
    "**Classification and evaluation (3 points)** Using these new document representations, apply <ins>three classification algorithms</ins> and report the evaluation results (based on accuracy metric) on the test set.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-taskC\"></a><h2 style=\"color:rgb(0,120,170)\">Task C: Classification with sent2vec Document Embeddings (2 extra point)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Sent2vec [2] suggests another unsupervised approach to creating document embeddings from the underlying word embeddings. First, using the provided code in the paper, train a sendtvec model on the training set to create document embeddings. Then, repeat Task B while using the document embeddings provided by sent2vec. Similar to Task 2, conduct the classification experiments and report evaluation results.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-references\"></a><h2 style=\"color:rgb(0,120,170)\">References</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] O. Levy, Y. Goldberg, and I. Dagan. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211– 225, 2015.\n",
    "\n",
    "[2] M. Pagliardini, P. Gupta, and M. Jaggi. Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features. In Proceedings of the conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
